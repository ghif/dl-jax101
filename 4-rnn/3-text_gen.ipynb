{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generasi Teks dengan RNN/LSTM menggunakan JAX dan Flax NNX\n",
                "\n",
                "Notebook ini mendemonstrasikan cara membangun model bahasa sederhana untuk menghasilkan teks secara otomatis (text generation) menggunakan library JAX dan Flax NNX. Kita akan menggunakan kumpulan puisi Chairil Anwar sebagai data latih.\n",
                "\n",
                "## Langkah 1: Persiapan Lingkungan dan Konfigurasi\n",
                "\n",
                "Pertama, kita perlu mengatur path agar notebook dapat menemukan modul pendukung (`model_utils` dan `seq_processor`) serta mengonfigurasi JAX untuk berjalan di CPU guna menghindari masalah kompatibilitas pada beberapa perangkat Mac."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import optax\n",
                "import numpy as np\n",
                "from time import process_time\n",
                "\n",
                "# Mengatur agar path mengarah ke root directory proyek JAX\n",
                "script_dir = os.getcwd()\n",
                "jax_dir = os.path.dirname(script_dir)\n",
                "if jax_dir not in sys.path:\n",
                "    sys.path.append(jax_dir)\n",
                "\n",
                "import seq_processor as sp\n",
                "import model_utils as mu\n",
                "\n",
                "# # Paksa penggunaan CPU untuk stabilitas\n",
                "# jax.config.update(\"jax_platform_name\", \"cpu\")\n",
                "\n",
                "# print(\"JAX Platform:\", jax.lib.xla_bridge.get_backend().platform)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 2: Memuat dan Memproses Data\n",
                "\n",
                "Kita akan menggunakan file teks berisi puisi-puisi Chairil Anwar. Karakter-karakter dalam teks tersebut akan diubah menjadi representasi numerik (integer) agar dapat diproses oleh model neural network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Length of text: 37970 characters\n",
                        "\n",
                        " !&()*+,-.0123456789:;?ABCDEFGHIJKLMNOPRSTUWYabcdefghijklmnoprstuvwxyzé–‘’“”…\n",
                        "78\n",
                        "Total karakter: 37970\n",
                        "Ukuran vokabulari: 78\n"
                    ]
                }
            ],
            "source": [
                "data_dir = \"../data\"\n",
                "data_path = os.path.join(data_dir, \"chairilanwar.txt\")\n",
                "\n",
                "# Inisialisasi processor karakter\n",
                "chproc = sp.CharProcessor(data_path)\n",
                "data = jnp.array(chproc.encode(chproc.text), dtype=jnp.int32)\n",
                "\n",
                "print(f\"Total karakter: {len(data)}\")\n",
                "print(f\"Ukuran vokabulari: {chproc.vocab_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 3: Definisi Model dan Optimizer\n",
                "\n",
                "Kita menggunakan arsitektur `SimpleBigram` yang didasarkan pada LSTM. Model ini telah dioptimasi menggunakan `jax.lax.scan` untuk menangani urutan karakter yang panjang secara efisien tanpa memperlambat proses kompilasi JIT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model siap dilatih.\n"
                    ]
                }
            ],
            "source": [
                "seq_len = 256\n",
                "n_embed = 384\n",
                "n_hidden = 512\n",
                "batch_size = 64\n",
                "\n",
                "# Inisialisasi model\n",
                "rngs = nnx.Rngs(1337)\n",
                "model = mu.SimpleBigram(\n",
                "    chproc.vocab_size,\n",
                "    seq_len,\n",
                "    n_embed,\n",
                "    n_hidden,\n",
                "    num_layers=1,\n",
                "    rngs=rngs\n",
                ")\n",
                "\n",
                "# Inisialisasi optimizer dengan AdamW\n",
                "optimizer = nnx.Optimizer(model, optax.adamw(3e-4), wrt=nnx.Param)\n",
                "\n",
                "print(\"Model siap dilatih.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 4: Fungsi Pelatihan\n",
                "\n",
                "Kita mendefinisikan `loss_fn` untuk menghitung error (cross-entropy) dan `train_step` yang dihiasi dengan `@nnx.jit` untuk mengeksekusi pelatihan secara cepat di XLA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss_fn(model, xb, yb):\n",
                "    logits = model(xb)\n",
                "    B, T, C = logits.shape\n",
                "    logits_flat = logits.reshape(B * T, C)\n",
                "    targets_flat = yb.reshape(B * T)\n",
                "    loss = optax.softmax_cross_entropy_with_integer_labels(logits_flat, targets_flat).mean()\n",
                "    return loss\n",
                "\n",
                "@nnx.jit\n",
                "def train_step(model, optimizer, xb, yb):\n",
                "    loss, grads = nnx.value_and_grad(loss_fn)(model, xb, yb)\n",
                "    optimizer.update(model, grads)\n",
                "    return loss\n",
                "\n",
                "@nnx.jit(static_argnums=(2, 3, 4))\n",
                "def estimate_loss(model, data, eval_iters=10, batch_size=32, seq_len=64, key=None):\n",
                "    model.eval()\n",
                "    losses = []\n",
                "    for k in range(eval_iters):\n",
                "        curr_key = jax.random.fold_in(key, k) if key is not None else jax.random.PRNGKey(k)\n",
                "        xb, yb = sp.get_batch(data, batch_size=batch_size, block_size=seq_len, key=curr_key)\n",
                "        loss = loss_fn(model, xb, yb)\n",
                "        losses.append(loss)\n",
                "    \n",
                "    avg_loss = jnp.mean(jnp.array(losses))\n",
                "    model.train()\n",
                "    return avg_loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 5: Proses Pelatihan\n",
                "\n",
                "Kita akan menjalankan iterasi pelatihan. Setiap interval tertentu, kita akan menghitung rata-rata loss dan mencoba menghasilkan beberapa teks awal untuk melihat perkembangan model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Memulai pelatihan...\n",
                        "[Iter-1/100] Loss: 4.3447 (1.011s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        "()SrDPg6G27fWfjhp1n.A’lRKv*;dn?5 SEcf*P6LHO;?EKcjB\n",
                        "------------------------\n",
                        "[Iter-21/100] Loss: 3.2492 (0.006s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        "-NiY2’y5\n",
                        "!FiB eaakk0nnln, tB\n",
                        "jaL*tPhdpuuvi\n",
                        "nueti i\n",
                        "------------------------\n",
                        "[Iter-41/100] Loss: 3.1409 (0.005s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        ".c;Sls,+pPxp alodpjmdEeedbi: u AgbrbiseB a  rdrniN\n",
                        "------------------------\n",
                        "[Iter-61/100] Loss: 3.0425 (0.004s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        "z…uI–4PhA ijp\n",
                        "ra K d Klhu eda :riaa\n",
                        "a\n",
                        " i\n",
                        "dDseii  a\n",
                        "------------------------\n",
                        "[Iter-81/100] Loss: 2.8326 (0.003s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        "mjYvzlNbkm regatnmbkK 4ami\n",
                        "ulanaearsa\n",
                        "i  Skaldkdli\n",
                        "------------------------\n",
                        "[Iter-100/100] Loss: 2.5986 (0.003s)\n",
                        "--- Teks Tergenerasi ---\n",
                        "\n",
                        "IbHUdAld iamnna\n",
                        "hitadu\n",
                        " jabAc\n",
                        "K\n",
                        "mAtan.e menyantgaa\n",
                        "------------------------\n"
                    ]
                }
            ],
            "source": [
                "max_iters = 100 # Dikurangi untuk demonstrasi cepat\n",
                "eval_interval = 20\n",
                "key = jax.random.PRNGKey(0)\n",
                "\n",
                "print(\"Memulai pelatihan...\")\n",
                "\n",
                "for step in range(max_iters):\n",
                "    key, subkey = jax.random.split(key)\n",
                "    xb, yb = sp.get_batch(data, batch_size=batch_size, block_size=seq_len, key=subkey)\n",
                "\n",
                "    start_t = process_time()\n",
                "    loss = train_step(model, optimizer, xb, yb)\n",
                "    elapsed_t = process_time() - start_t\n",
                "\n",
                "    if step % eval_interval == 0 or step == max_iters - 1:\n",
                "        key, subkey = jax.random.split(key)\n",
                "        train_loss = estimate_loss(model, data, eval_iters=5, batch_size=batch_size, seq_len=seq_len, key=subkey)\n",
                "\n",
                "        print(f\"[Iter-{step+1}/{max_iters}] Loss: {train_loss:.4f} ({elapsed_t:.3f}s)\")\n",
                "        \n",
                "        # Coba hasilkan teks singkat\n",
                "        idx = jnp.zeros((1, 1), dtype=jnp.int32)\n",
                "        pred_idx = model.generate(idx, 50, rngs=rngs)\n",
                "        pred_str = chproc.decode(np.array(pred_idx[0]))\n",
                "        print(f\"--- Teks Tergenerasi ---\\n{pred_str}\\n------------------------\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 6: Generasi Teks Akhir\n",
                "\n",
                "Setelah model dilatih, kita bisa menghasilkan teks yang lebih panjang."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating long text samples...\n",
                        "Final Generated text:\n",
                        "\n",
                        "\n",
                        "yL.sk2 –\n",
                        "rer\n",
                        "sH kan temakupadDa,Ta tahuMb 1Albudan bai darhi\n",
                        " meatukaka ippamean asRtu aenp,  etNu tatu. iiw d lan dak sendi auiuu 1enbasinlan gadi \n",
                        "ahgiJus aeda btua L?denip\n",
                        "hbuu p nunga\n",
                        "garen yehu.a\n"
                    ]
                }
            ],
            "source": [
                "print(\"Generating long text samples...\")\n",
                "idx = jnp.zeros((1, 1), dtype=jnp.int32)\n",
                "pred_idx = model.generate(idx, 200, rngs=rngs)\n",
                "pred_str = chproc.decode(np.array(pred_idx[0]))\n",
                "print(f\"Final Generated text:\\n\\n{pred_str}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax-cpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
