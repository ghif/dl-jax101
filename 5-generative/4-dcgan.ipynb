{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Convolutional Generative Adversarial Network (DCGAN) dengan JAX/Flax NNX\n",
                "\n",
                "Notebook ini akan memandu Anda melalui implementasi **DCGAN** menggunakan pustaka **JAX** dan **Flax NNX**. DCGAN adalah salah satu varian GAN yang menggunakan lapisan konvolusi untuk membangkitkan gambar yang realistis ([Radford et al. 2015](https://arxiv.org/abs/1511.06434)).\n",
                "\n",
                "### Apa itu GAN?\n",
                "Generative Adversarial Networks (GANs) terdiri dari dua model yang saling bersaing:\n",
                "1.  **Generator**: Belajar membangkitkan gambar yang terlihat \"asli\" dari vektor noise (latent vector).\n",
                "2.  **Discriminator**: Belajar membedakan antara gambar asli (dari dataset) dan gambar palsu (yang dibuat oleh Generator).\n",
                "\n",
                "Selama pelatihan, Generator berusaha menipu Discriminator, sementara Discriminator berusaha menjadi lebih pintar dalam mendeteksi gambar palsu. Persaingan ini mendorong Generator untuk menghasilkan gambar yang semakin realistis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Persiapan Lingkungan dan Import Pustaka\n",
                "\n",
                "Pertama, kita perlu mengimpor semua modul yang diperlukan. Kita menggunakan `flax.nnx` untuk mendefinisikan model secara objek-oriented (serupa dengan PyTorch) namun tetap mempertahankan kekuatan fungsional JAX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "JAX Device: [CpuDevice(id=0)]\n"
                    ]
                }
            ],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import matplotlib.pyplot as plt\n",
                "import sys, os\n",
                "import numpy as np\n",
                "import time as timer\n",
                "from tqdm import tqdm\n",
                "import grain.python as grain\n",
                "import optax\n",
                "import urllib.request\n",
                "import tarfile\n",
                "import pickle\n",
                "\n",
                "# Memasukkan parent directory agar bisa mengimpor utils\n",
                "script_dir = os.path.abspath('')\n",
                "parent_dir = os.path.dirname(script_dir)\n",
                "if parent_dir not in sys.path:\n",
                "    sys.path.append(parent_dir)\n",
                "\n",
                "import viz_utils as vu\n",
                "import model_utils as mu\n",
                "\n",
                "print(f\"JAX Device: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pengaturan Konstanta\n",
                "\n",
                "Definisikan parameter hyper-parameter untuk pelatihan, seperti ukuran batch (`BATCH_SIZE`), dimensi laten (`NZ`), dan learning rate (`LR`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = os.path.join(parent_dir, \"data\")\n",
                "MODEL_DIR = os.path.join(parent_dir, \"models\") \n",
                "\n",
                "BATCH_SIZE = 64\n",
                "NUM_EPOCH = 50\n",
                "IMAGE_SIZE = 64\n",
                "NC = 3 # RGB untuk CIFAR-10\n",
                "NZ = 100 \n",
                "NGF = 64 \n",
                "NDF = 64 \n",
                "LR = 1e-4 \n",
                "BETA1 = 0.5 \n",
                "NVIZ = 64\n",
                "\n",
                "DATASET = 'cifar10'\n",
                "checkpoint_dir = os.path.join(MODEL_DIR, f\"dcgan_{DATASET}_z{NZ}_v2\")\n",
                "sample_dir = os.path.join(checkpoint_dir, \"samples\")\n",
                "\n",
                "for d in [checkpoint_dir, sample_dir, DATA_DIR, \n",
                "          os.path.join(checkpoint_dir, \"generator\"), \n",
                "          os.path.join(checkpoint_dir, \"discriminator\")]:\n",
                "    if not os.path.exists(d):\n",
                "        os.makedirs(d)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Persiapan Data (CIFAR-10)\n",
                "\n",
                "Mengunduh dataset CIFAR-10 dan menyiapkannya untuk pelatihan. Kita menggunakan `grain` untuk loading data yang efisien."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_and_extract_cifar10(dest_dir):\n",
                "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
                "    filename = os.path.join(dest_dir, \"cifar-10-python.tar.gz\")\n",
                "    extract_path = os.path.join(dest_dir, \"cifar-10-batches-py\")\n",
                "    \n",
                "    if not os.path.exists(extract_path):\n",
                "        if not os.path.exists(filename):\n",
                "            print(f\"Downloading {url}...\")\n",
                "            urllib.request.urlretrieve(url, filename)\n",
                "            print(\"Download complete.\")\n",
                "        \n",
                "        print(f\"Extracting {filename}...\")\n",
                "        with tarfile.open(filename, \"r:gz\") as tar:\n",
                "            tar.extractall(path=dest_dir)\n",
                "        print(\"Extraction complete.\")\n",
                "    return extract_path\n",
                "\n",
                "def load_cifar10_local(data_dir):\n",
                "    def unpickle(file):\n",
                "        with open(file, 'rb') as fo:\n",
                "            dict = pickle.load(fo, encoding='bytes')\n",
                "        return dict\n",
                "\n",
                "    images = []\n",
                "    labels = []\n",
                "    for i in range(1, 6):\n",
                "        batch_file = os.path.join(data_dir, f\"data_batch_{i}\")\n",
                "        batch = unpickle(batch_file)\n",
                "        images.append(batch[b'data'])\n",
                "        labels.append(batch[b'labels'])\n",
                "    \n",
                "    X_train = np.vstack(images)\n",
                "    y_train = np.hstack(labels).astype(np.int32)\n",
                "    return X_train, y_train\n",
                "\n",
                "print(f\"Loading {DATASET} locally...\")\n",
                "cifar_path = download_and_extract_cifar10(DATA_DIR)\n",
                "X_train_all, y_train_all = load_cifar10_local(cifar_path)\n",
                "print(f\"Dataset loaded. X_train_all shape: {X_train_all.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Loader dengan Grain\n",
                "\n",
                "Kita mendefinisikan `CIFARSource` untuk mengubah format data mentah menjadi gambar yang ternormalisasi ([-1, 1]) dan berukuran 64x64."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CIFARSource(grain.RandomAccessDataSource):\n",
                "    def __init__(self, images, labels):\n",
                "        self._images = images\n",
                "        self._labels = labels\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self._images)\n",
                "        \n",
                "    def __getitem__(self, index):\n",
                "        from PIL import Image\n",
                "        # CIFAR-10 flattened (3072,) -> (3, 32, 32) -> transpose to (32, 32, 3)\n",
                "        img = self._images[index].reshape(3, 32, 32).transpose(1, 2, 0).astype(np.uint8)\n",
                "        img = Image.fromarray(img)\n",
                "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR)\n",
                "        img = np.array(img).astype(np.float32)\n",
                "        # Normalisasi ke [-1, 1] - Sangat penting untuk GAN dengan Tanh output\n",
                "        image = (img / 255.0) * 2.0 - 1.0\n",
                "        label = self._labels[index]\n",
                "        return {'image': image, 'label': label}\n",
                "\n",
                "def create_loader(data_source, batch_size, shuffle=False, seed=0):\n",
                "    sampler = grain.IndexSampler(\n",
                "        num_records=len(data_source),\n",
                "        shard_options=grain.NoSharding(),\n",
                "        shuffle=shuffle,\n",
                "        num_epochs=1,\n",
                "        seed=seed,\n",
                "    )\n",
                "    dataloader = grain.DataLoader(\n",
                "        data_source=data_source,\n",
                "        sampler=sampler,\n",
                "        worker_count=0,\n",
                "    )\n",
                "    \n",
                "    class BatchIterator:\n",
                "        def __init__(self, loader, batch_size, num_records):\n",
                "            self.loader = loader\n",
                "            self.batch_size = batch_size\n",
                "            self.num_records = num_records\n",
                "        def __len__(self):\n",
                "            return (self.num_records + self.batch_size - 1) // self.batch_size\n",
                "        def __iter__(self):\n",
                "            batch_images, batch_labels = [], []\n",
                "            for record in self.loader:\n",
                "                batch_images.append(record['image'])\n",
                "                batch_labels.append(record['label'])\n",
                "                if len(batch_images) == self.batch_size:\n",
                "                    yield np.stack(batch_images), np.array(batch_labels)\n",
                "                    batch_images, batch_labels = [], []\n",
                "            if batch_images:\n",
                "                 yield np.stack(batch_images), np.array(batch_labels)\n",
                "    return BatchIterator(dataloader, batch_size, len(data_source))\n",
                "\n",
                "train_loader = create_loader(CIFARSource(X_train_all, y_train_all), BATCH_SIZE, shuffle=True, seed=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Definisi Model\n",
                "\n",
                "### Generator\n",
                "Generator bertugas memetakan vektor noise $z$ ke ruang citra. Model ini menggunakan `ConvTranspose` untuk melakukan *upsampling* dari tensor 1x1 ke 64x64."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, rngs: nnx.Rngs):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        \n",
                "        # Input: (N, 1, 1, NZ)\n",
                "        # Output: (N, 4, 4, ngf * 8)\n",
                "        self.convt1 = nnx.ConvTranspose(nz, ngf * 8, kernel_size=(4, 4), strides=(1, 1), padding='VALID', \n",
                "                                        use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn1 = nnx.BatchNorm(ngf * 8, rngs=rngs)\n",
                "        \n",
                "        # Output: (N, 8, 8, ngf * 4)\n",
                "        self.convt2 = nnx.ConvTranspose(ngf * 8, ngf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                                        use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn2 = nnx.BatchNorm(ngf * 4, rngs=rngs)\n",
                "        \n",
                "        # Output: (N, 16, 16, ngf * 2)\n",
                "        self.convt3 = nnx.ConvTranspose(ngf * 4, ngf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                                        use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn3 = nnx.BatchNorm(ngf * 2, rngs=rngs)\n",
                "\n",
                "        # Output: (N, 32, 32, ngf)\n",
                "        self.convt4 = nnx.ConvTranspose(ngf * 2, ngf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                                        use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn4 = nnx.BatchNorm(ngf, rngs=rngs)\n",
                "        \n",
                "        # Output: (N, 64, 64, nc)\n",
                "        self.convt5 = nnx.ConvTranspose(ngf, nc, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                                        use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "\n",
                "    def __call__(self, z, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None:\n",
                "            use_running_average = not train\n",
                "        # Reshape noise z (N, NZ) ke (N, 1, 1, NZ)\n",
                "        h = z.reshape(z.shape[0], 1, 1, -1)\n",
                "        h = nnx.relu(self.bn1(self.convt1(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn2(self.convt2(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn3(self.convt3(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn4(self.convt4(h), use_running_average=use_running_average))\n",
                "        return nnx.tanh(self.convt5(h))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Discriminator\n",
                "Discriminator adalah jaringan pengevaluasi yang bertugas mengklasifikasikan apakah sebuah gambar itu nyata atau palsu. Model ini menggunakan `Conv` biasa untuk *downsampling*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Discriminator(nnx.Module):\n",
                "    def __init__(self, nc, ndf, rngs: nnx.Rngs):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        \n",
                "        self.conv1 = nnx.Conv(nc, ndf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                              use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.conv2 = nnx.Conv(ndf, ndf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                              use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn2 = nnx.BatchNorm(ndf * 2, rngs=rngs)\n",
                "        self.conv3 = nnx.Conv(ndf * 2, ndf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                              use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn3 = nnx.BatchNorm(ndf * 4, rngs=rngs)\n",
                "        self.conv4 = nnx.Conv(ndf * 4, ndf * 8, kernel_size=(4, 4), strides=(2, 2), padding='SAME', \n",
                "                              use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn4 = nnx.BatchNorm(ndf * 8, rngs=rngs)\n",
                "        self.conv5 = nnx.Conv(ndf * 8, 1, kernel_size=(4, 4), strides=(1, 1), padding='VALID', \n",
                "                              use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "\n",
                "    def __call__(self, x, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None:\n",
                "            use_running_average = not train\n",
                "        h = nnx.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn2(self.conv2(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn3(self.conv3(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn4(self.conv4(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        return self.conv5(h).flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DCGAN Container\n",
                "Kita satukan kedua model dalam satu kelas `DCGAN` untuk memudahkan manajemen."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DCGAN(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, ndf, rngs: nnx.Rngs):\n",
                "        self.netG = Generator(nz, ngf, nc, rngs)\n",
                "        self.netD = Discriminator(nc, ndf, rngs)\n",
                "        \n",
                "rngs = nnx.Rngs(0)\n",
                "model = DCGAN(NZ, NGF, NC, NDF, rngs=rngs)\n",
                "optimizerG = nnx.Optimizer(model.netG, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "optimizerD = nnx.Optimizer(model.netD, optax.adam(LR, b1=BETA1), wrt=nnx.Param)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Fungsi Pelatihan\n",
                "\n",
                "### Loss Function\n",
                "Kita menggunakan Binary Cross Entropy (BCE) sebagai pengukur kesalahan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss_bce(logits, labels):\n",
                "    return jnp.mean(optax.sigmoid_binary_cross_entropy(logits, labels))\n",
                "\n",
                "def update_optimizer(optimizer, module, grads):\n",
                "    try:\n",
                "        optimizer.update(module, grads)\n",
                "    except TypeError:\n",
                "        optimizer.update(grads)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step Pelatihan Discriminator dan Generator\n",
                "\n",
                "Penting untuk melakukan `stop_gradient` pada gambar palsu saat melatih Discriminator agar gradient tidak mengalir balik ke Generator."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@nnx.jit\n",
                "def train_step_D(model, optimizerD, real_x, noise):\n",
                "    fake_x = model.netG(noise, train=True) \n",
                "    fake_x = jax.lax.stop_gradient(fake_x)\n",
                "\n",
                "    def loss_fn(model):\n",
                "        real_logits = model.netD(real_x, train=True)\n",
                "        fake_logits = model.netD(fake_x, train=True)\n",
                "        \n",
                "        errD_real = loss_bce(real_logits, jnp.ones_like(real_logits))\n",
                "        errD_fake = loss_bce(fake_logits, jnp.zeros_like(fake_logits))\n",
                "        errD = errD_real + errD_fake\n",
                "        \n",
                "        real_p = nnx.sigmoid(real_logits)\n",
                "        fake_p = nnx.sigmoid(fake_logits)\n",
                "        return errD, (real_p, fake_p)\n",
                "        \n",
                "    (loss, (real_p, fake_p)), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    update_optimizer(optimizerD, model.netD, grads.netD)\n",
                "    return loss, jnp.mean(real_p), jnp.mean(fake_p)\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_G(model, optimizerG, noise):\n",
                "    def loss_fn(model):\n",
                "        fake_x = model.netG(noise, train=True)\n",
                "        fake_logits = model.netD(fake_x, train=True)\n",
                "        # Generator ingin Discriminator percaya bahwa gambar ini asli (label 1)\n",
                "        errG = loss_bce(fake_logits, jnp.ones_like(fake_logits))\n",
                "        return errG, nnx.sigmoid(fake_logits)\n",
                "    (loss, outD), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    update_optimizer(optimizerG, model.netG, grads.netG)\n",
                "    return loss, jnp.mean(outD)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Loop Pelatihan Utama\n",
                "\n",
                "Di sini kita menjalankan proses pelatihan selama beberapa epoch. Kita juga akan menyimpan sampel gambar secara berkala untuk melihat kemajuan Generator."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting Training Loop...\")\n",
                "fixed_latent = jax.random.normal(jax.random.PRNGKey(42), (NVIZ, NZ))\n",
                "step_rng = jax.random.PRNGKey(0)\n",
                "\n",
                "for epoch in range(NUM_EPOCH):\n",
                "    start_t = timer.time()\n",
                "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}\") as tepoch:\n",
                "        for batch_idx, (real_x, _) in enumerate(tepoch):\n",
                "            step_rng, rng_d, rng_g = jax.random.split(step_rng, 3)\n",
                "            noise_d = jax.random.normal(rng_d, (real_x.shape[0], NZ))\n",
                "            errD, D_x, D_G_z1 = train_step_D(model, optimizerD, real_x, noise_d)\n",
                "            noise_g = jax.random.normal(rng_g, (real_x.shape[0], NZ))\n",
                "            errG, D_G_z2 = train_step_G(model, optimizerG, noise_g)\n",
                "            \n",
                "            if batch_idx % 10 == 0:\n",
                "                tepoch.set_postfix(Loss_D=f\"{errD:.4f}\", Loss_G=f\"{errG:.4f}\", Dx=f\"{D_x:.4f}\", Dgz=f\"{D_G_z2:.4f}\")\n",
                "\n",
                "    # Visualisasi dan simpan sampel tiap epoch\n",
                "    fake_samples = model.netG(fixed_latent, train=False)\n",
                "    grid = vu.set_grid(fake_samples, num_cells=NVIZ)\n",
                "    \n",
                "    plt.figure(figsize=(8, 8))\n",
                "    # Convert to NHWC for plotting as JAX set_grid might return CHW\n",
                "    img_grid = np.transpose(np.array(vu.normalize(grid, 0, 1)), (1, 2, 0))\n",
                "    plt.imshow(img_grid)\n",
                "    plt.axis('off')\n",
                "    plt.title(f\"Epoch {epoch+1}\")\n",
                "    plt.show() # Tampilkan di notebook\n",
                "    \n",
                "    # Simpan ke file\n",
                "    plt.imsave(os.path.join(sample_dir, f'samples_epoch_{epoch+1}.png'), img_grid)\n",
                "    \n",
                "    # Simpan checkpoint\n",
                "    mu.save_checkpoint(model.netG, epoch + 1, filedir=os.path.join(checkpoint_dir, \"generator\"))\n",
                "    mu.save_checkpoint(model.netD, epoch + 1, filedir=os.path.join(checkpoint_dir, \"discriminator\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Kesimpulan\n",
                "\n",
                "Dalam tutorial ini, kita telah:\n",
                "1. Membangun Generator dan Discriminator menggunakan **Flax NNX**.\n",
                "2. Menyiapkan pipeline data menggunakan **Grain**.\n",
                "3. Mengimplementasikan update gradient yang terpisah untuk model adversarial.\n",
                "4. Melatih model pada dataset CIFAR-10.\n",
                "\n",
                "Anda bisa mencoba mengganti `NUM_EPOCH` yang lebih tinggi atau mengubah arsitektur model untuk mendapatkan hasil yang lebih tajam!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax-cpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
