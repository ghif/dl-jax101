{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conditional Generative Adversarial Network (cGAN) dengan JAX/Flax (GCS Edition)\n",
                "\n",
                "Notebook ini mendemonstrasikan implementasi **Conditional Generative Adversarial Network (cGAN)** menggunakan framework **JAX** dan library **Flax (NNX)**. Kita akan melatih model ini pada dataset **CIFAR-10** dan menyimpan hasilnya langsung ke **Google Cloud Storage (GCS)**."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Perbedaan antara Conditional GAN (cGAN) dan DCGAN\n",
                "\n",
                "| Fitur | DCGAN | Conditional GAN (cGAN) |\n",
                "|---|---|---|\n",
                "| **Input Generator** | Noise acak ($z$). | Noise acak ($z$) + Label kelas ($y$). |\n",
                "| **Input Discriminator** | Gambar ($x$). | Gambar ($x$) + Label kelas ($y$). |\n",
                "| **Kontrol Output** | Acak. | Terkendali (berdasarkan label). |\n",
                "\n",
                "**cGAN** memungkinkan kita untuk memandu proses pembangkitan gambar dengan memberikan label sebagai input tambahan."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Persiapan Lingkungan, Import, dan GCS Auth\n",
                "\n",
                "Kita siapkan library dan autentikasi ke Google Cloud Storage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "JAX Device: [CudaDevice(id=0)]\n"
                    ]
                }
            ],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import matplotlib.pyplot as plt\n",
                "import sys, os\n",
                "import numpy as np\n",
                "import time as timer\n",
                "from tqdm import tqdm\n",
                "import grain.python as grain\n",
                "import optax\n",
                "import urllib.request\n",
                "import tarfile\n",
                "import pickle\n",
                "import safetensors\n",
                "from safetensors.flax import save_file\n",
                "\n",
                "from google.colab import auth\n",
                "from google.cloud import storage\n",
                "\n",
                "# Autentikasi GCS\n",
                "auth.authenticate_user()\n",
                "storage_client = storage.Client()\n",
                "BUCKET_NAME = 'dljax'\n",
                "bucket = storage_client.bucket(BUCKET_NAME)\n",
                "\n",
                "print(f\"JAX Device: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GCS Helpers\n",
                "def upload_to_gcs(local_path, gcs_path):\n",
                "    blob = bucket.blob(gcs_path)\n",
                "    blob.upload_from_filename(local_path)\n",
                "    # print(f\"Uploaded {local_path} to gs://{BUCKET_NAME}/{gcs_path}\")\n",
                "\n",
                "# Visualization helpers\n",
                "def set_grid(D, num_cells=1):\n",
                "    if len(D.shape) == 3:\n",
                "        n, h, w = D.shape\n",
                "        D = D[:, jnp.newaxis, :, :]\n",
                "    \n",
                "    if D.shape[1] in [1, 3] and D.shape[3] not in [1, 3]:\n",
                "        n, c, d1, d2 = D.shape\n",
                "    elif D.shape[3] in [1, 3]:\n",
                "        D = jnp.transpose(D, (0, 3, 1, 2))\n",
                "        n, c, d1, d2 = D.shape\n",
                "    else:\n",
                "        n, c, d1, d2 = D.shape\n",
                "    \n",
                "    grid_size = int(jnp.ceil(jnp.sqrt(num_cells)))\n",
                "    grid = jnp.zeros((c, grid_size * d1, grid_size * d2))\n",
                "    \n",
                "    for i in range(num_cells):\n",
                "        if i >= n: break\n",
                "        r = i // grid_size\n",
                "        col = i % grid_size\n",
                "        grid = grid.at[:, r*d1:(r+1)*d1, col*d2:(col+1)*d2].set(D[i])\n",
                "        \n",
                "    return grid\n",
                "\n",
                "def normalize(x, new_min=0, new_max=255):\n",
                "    old_min = np.min(x)\n",
                "    old_max = np.max(x)\n",
                "    xn = (x - old_min) * ((new_max - new_min) / (old_max - old_min)) + new_min\n",
                "    return xn\n",
                "\n",
                "# Checkpoint helper with GCS Auto-Upload\n",
                "def save_checkpoint(model: nnx.Module, epoch: int, filedir: str = \"checkpoint\", gcs_prefix: str = \"models\"):\n",
                "    _, state = nnx.split(model)\n",
                "    flat_state = state.to_pure_dict()\n",
                "    \n",
                "    def flatten_dict(d, parent_key='', sep='.'):\n",
                "        items = []\n",
                "        for k, v in d.items():\n",
                "            if v is None: continue\n",
                "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
                "            if isinstance(v, dict):\n",
                "                items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
                "            else:\n",
                "                items.append((new_key, v))\n",
                "        return dict(items)\n",
                "    \n",
                "    flat_params = flatten_dict(flat_state)\n",
                "    \n",
                "    filename = f\"epoch_{epoch}.safetensors\"\n",
                "    local_dir = os.path.join(\"/content\", filedir)\n",
                "    os.makedirs(local_dir, exist_ok=True)\n",
                "    local_path = os.path.join(local_dir, filename)\n",
                "    save_file(flat_params, local_path)\n",
                "    \n",
                "    # Upload to GCS\n",
                "    gcs_path = f\"{gcs_prefix}/{filedir}/{filename}\"\n",
                "    upload_to_gcs(local_path, gcs_path)\n",
                "    print(f\"Model saved locally to {local_path} and uploaded to gs://{BUCKET_NAME}/{gcs_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hyperparameters dan Direktori\n",
                "\n",
                "Kita definisikan konstanta yang akan digunakan selama eksperimen."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 128\n",
                "NUM_EPOCH = 50\n",
                "IMAGE_SIZE = 64\n",
                "NC = 3\n",
                "NZ = 100\n",
                "NGF = 64\n",
                "NDF = 64\n",
                "LR = 2e-4 \n",
                "BETA1 = 0.5 \n",
                "NVIZ = 64\n",
                "NUM_CLASSES = 10\n",
                "\n",
                "DATA_DIR = \"/content/data\"\n",
                "SAMPLE_DIR = \"samples\"\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(SAMPLE_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset: CIFAR-10\n",
                "\n",
                "Fungsi-fungsi di bawah ini digunakan untuk mengunduh dan memuat dataset CIFAR-10 secara lokal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_and_extract_cifar10(dest_dir):\n",
                "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
                "    filename = os.path.join(dest_dir, \"cifar-10-python.tar.gz\")\n",
                "    extract_path = os.path.join(dest_dir, \"cifar-10-batches-py\")\n",
                "    \n",
                "    if not os.path.exists(extract_path):\n",
                "        if not os.path.exists(filename):\n",
                "            print(f\"Downloading {url}...\")\n",
                "            urllib.request.urlretrieve(url, filename)\n",
                "        \n",
                "        with tarfile.open(filename, \"r:gz\") as tar:\n",
                "            tar.extractall(path=dest_dir)\n",
                "    return extract_path\n",
                "\n",
                "def load_cifar10_local(data_dir):\n",
                "    def unpickle(file):\n",
                "        with open(file, 'rb') as fo:\n",
                "            d = pickle.load(fo, encoding='bytes')\n",
                "        return d\n",
                "\n",
                "    images, labels = [], []\n",
                "    for i in range(1, 6):\n",
                "        batch = unpickle(os.path.join(data_dir, f\"data_batch_{i}\"))\n",
                "        images.append(batch[b'data'])\n",
                "        labels.append(batch[b'labels'])\n",
                "    \n",
                "    X_train = np.vstack(images)\n",
                "    y_train = np.hstack(labels).astype(np.int32)\n",
                "    return X_train, y_train\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "cifar_path = download_and_extract_cifar10(DATA_DIR)\n",
                "X_train_all, y_train_all = load_cifar10_local(cifar_path)\n",
                "print(f\"Data loaded: {X_train_all.shape} images.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pipeline Data dengan Google Grain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CIFARSource(grain.RandomAccessDataSource):\n",
                "    def __init__(self, images, labels):\n",
                "        self._images = images\n",
                "        self._labels = labels\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self._images)\n",
                "        \n",
                "    def __getitem__(self, index):\n",
                "        from PIL import Image\n",
                "        img = self._images[index].reshape(3, 32, 32).transpose(1, 2, 0).astype(np.uint8)\n",
                "        img = Image.fromarray(img).resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR)\n",
                "        img = np.array(img).astype(np.float32)\n",
                "        image = (img / 255.0) * 2.0 - 1.0\n",
                "        return {'image': image, 'label': self._labels[index]}\n",
                "\n",
                "def create_loader(data_source, batch_size, shuffle=False, seed=0):\n",
                "    sampler = grain.IndexSampler(num_records=len(data_source), shard_options=grain.NoSharding(), shuffle=shuffle, num_epochs=1, seed=seed)\n",
                "    dataloader = grain.DataLoader(data_source=data_source, sampler=sampler, worker_count=0)\n",
                "    \n",
                "    class BatchIterator:\n",
                "        def __init__(self, loader, batch_size, num_records): self.loader, self.batch_size, self.num_records = loader, batch_size, num_records\n",
                "        def __len__(self): return (self.num_records + self.batch_size - 1) // self.batch_size\n",
                "        def __iter__(self):\n",
                "            batch_images, batch_labels = [], []\n",
                "            for record in self.loader:\n",
                "                batch_images.append(record['image'])\n",
                "                batch_labels.append(record['label'])\n",
                "                if len(batch_images) == self.batch_size:\n",
                "                    yield np.stack(batch_images), np.array(batch_labels)\n",
                "                    batch_images, batch_labels = [], []\n",
                "            if batch_images: yield np.stack(batch_images), np.array(batch_labels)\n",
                "    return BatchIterator(dataloader, batch_size, len(data_source))\n",
                "\n",
                "train_loader = create_loader(CIFARSource(X_train_all, y_train_all), BATCH_SIZE, shuffle=True, seed=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Arsitektur Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, num_classes, rngs: nnx.Rngs):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        self.num_classes = num_classes\n",
                "        self.convt1 = nnx.ConvTranspose(nz + num_classes, ngf * 8, kernel_size=(4, 4), strides=(1, 1), padding='VALID', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn1 = nnx.BatchNorm(ngf * 8, rngs=rngs)\n",
                "        self.convt2 = nnx.ConvTranspose(ngf * 8, ngf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn2 = nnx.BatchNorm(ngf * 4, rngs=rngs)\n",
                "        self.convt3 = nnx.ConvTranspose(ngf * 4, ngf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn3 = nnx.BatchNorm(ngf * 2, rngs=rngs)\n",
                "        self.convt4 = nnx.ConvTranspose(ngf * 2, ngf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn4 = nnx.BatchNorm(ngf, rngs=rngs)\n",
                "        self.convt5 = nnx.ConvTranspose(ngf, nc, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "\n",
                "    def __call__(self, z, c, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None: use_running_average = not train\n",
                "        h = jnp.concatenate([z, c], axis=1).reshape(-1, 1, 1, z.shape[1] + c.shape[1])\n",
                "        h = nnx.relu(self.bn1(self.convt1(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn2(self.convt2(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn3(self.convt3(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn4(self.convt4(h), use_running_average=use_running_average))\n",
                "        return nnx.tanh(self.convt5(h))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Discriminator(nnx.Module):\n",
                "    def __init__(self, nc, ndf, num_classes, rngs: nnx.Rngs):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        self.conv1 = nnx.Conv(nc + num_classes, ndf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.conv2 = nnx.Conv(ndf, ndf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn2 = nnx.BatchNorm(ndf * 2, rngs=rngs)\n",
                "        self.conv3 = nnx.Conv(ndf * 2, ndf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn3 = nnx.BatchNorm(ndf * 4, rngs=rngs)\n",
                "        self.conv4 = nnx.Conv(ndf * 4, ndf * 8, kernel_size=(4, 4), strides=(2, 2), padding='SAME', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "        self.bn4 = nnx.BatchNorm(ndf * 8, rngs=rngs)\n",
                "        self.conv5 = nnx.Conv(ndf * 8, 1, kernel_size=(4, 4), strides=(1, 1), padding='VALID', use_bias=False, rngs=rngs, kernel_init=normal_init)\n",
                "\n",
                "    def __call__(self, x, c, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None: use_running_average = not train\n",
                "        c_spatial = jnp.broadcast_to(c[:, None, None, :], (x.shape[0], x.shape[1], x.shape[2], c.shape[1]))\n",
                "        h = jnp.concatenate([x, c_spatial], axis=-1)\n",
                "        h = nnx.leaky_relu(self.conv1(h), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn2(self.conv2(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn3(self.conv3(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn4(self.conv4(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        return self.conv5(h).flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Logic Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CGAN(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, ndf, num_classes, rngs: nnx.Rngs):\n",
                "        self.netG = Generator(nz, ngf, nc, num_classes, rngs)\n",
                "        self.netD = Discriminator(nc, ndf, num_classes, rngs)\n",
                "\n",
                "rngs = nnx.Rngs(0)\n",
                "model = CGAN(NZ, NGF, NC, NDF, NUM_CLASSES, rngs=rngs)\n",
                "optimizerG = nnx.Optimizer(model.netG, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "optimizerD = nnx.Optimizer(model.netD, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "\n",
                "def loss_bce(logits, labels): return jnp.mean(optax.sigmoid_binary_cross_entropy(logits, labels))\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_D(model, optimizerD, real_x, c_vec, noise):\n",
                "    fake_x = jax.lax.stop_gradient(model.netG(noise, c_vec, train=True))\n",
                "    def loss_fn(model):\n",
                "        real_logits, fake_logits = model.netD(real_x, c_vec, train=True), model.netD(fake_x, c_vec, train=True)\n",
                "        loss = loss_bce(real_logits, jnp.ones_like(real_logits)) + loss_bce(fake_logits, jnp.zeros_like(fake_logits))\n",
                "        return loss, (nnx.sigmoid(real_logits), nnx.sigmoid(fake_logits))\n",
                "    (loss, (real_p, fake_p)), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    optimizerD.update(model.netD, grads.netD)\n",
                "    return loss, jnp.mean(real_p), jnp.mean(fake_p)\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_G(model, optimizerG, c_vec, noise):\n",
                "    def loss_fn(model):\n",
                "        fake_logits = model.netD(model.netG(noise, c_vec, train=True), c_vec, train=True)\n",
                "        loss = loss_bce(fake_logits, jnp.ones_like(fake_logits))\n",
                "        return loss, nnx.sigmoid(fake_logits)\n",
                "    (loss, outD), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    optimizerG.update(model.netG, grads.netG)\n",
                "    return loss, jnp.mean(outD)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Loop Pelatihan (Main Loop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting Training Loop...\")\n",
                "step_rng = jax.random.PRNGKey(0)\n",
                "fixed_latent = jax.random.normal(jax.random.PRNGKey(42), (NVIZ, NZ))\n",
                "fixed_y = jnp.array([i % NUM_CLASSES for i in range(NVIZ)])\n",
                "fixed_cvec = jax.nn.one_hot(fixed_y, NUM_CLASSES)\n",
                "\n",
                "for epoch in range(NUM_EPOCH):\n",
                "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}\") as tepoch:\n",
                "        for batch_idx, (real_x, y) in enumerate(tepoch):\n",
                "            c_vec = jax.nn.one_hot(y, NUM_CLASSES)\n",
                "            step_rng, rng_d, rng_g = jax.random.split(step_rng, 3)\n",
                "            errD, D_x, D_G_z1 = train_step_D(model, optimizerD, real_x, c_vec, jax.random.normal(rng_d, (real_x.shape[0], NZ)))\n",
                "            errG, D_G_z2 = train_step_G(model, optimizerG, c_vec, jax.random.normal(rng_g, (real_x.shape[0], NZ)))\n",
                "            if batch_idx % 10 == 0: tepoch.set_postfix(Loss_D=f\"{errD:.4f}\", Loss_G=f\"{errG:.4f}\", Dx=f\"{D_x:.4f}\", Dgz=f\"{D_G_z2:.4f}\")\n",
                "\n",
                "    # Visualisasi dan simpan ke GCS\n",
                "    fake_samples = model.netG(fixed_latent, fixed_cvec, train=False)\n",
                "    grid = set_grid(fake_samples, num_cells=NVIZ)\n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.imshow(np.transpose(np.array(normalize(grid, 0, 1)), (1, 2, 0)))\n",
                "    plt.axis('off')\n",
                "    \n",
                "    sample_name = f'samples_epoch_{epoch+1}.png'\n",
                "    sample_path = os.path.join(SAMPLE_DIR, sample_name)\n",
                "    plt.savefig(sample_path)\n",
                "    upload_to_gcs(sample_path, f\"samples/{sample_name}\")\n",
                "    plt.show()\n",
                "\n",
                "    # Checkpointing ke GCS\n",
                "    save_checkpoint(model.netG, epoch + 1, filedir=\"generator\", gcs_prefix=\"models\")\n",
                "    save_checkpoint(model.netD, epoch + 1, filedir=\"discriminator\", gcs_prefix=\"models\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
