{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conditional Generative Adversarial Network (cGAN) dengan JAX/Flax (Colab and GCS Edition)\n",
                "\n",
                "Notebook ini mendemonstrasikan implementasi **Conditional Generative Adversarial Network (cGAN)** menggunakan framework **JAX** dan library **Flax (NNX)**. Versi ini dioptimalkan dengan **Mixed Precision (bfloat16)** untuk pelatihan yang lebih cepat pada GPU modern (seperti A100)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Perbedaan antara Conditional GAN (cGAN) dan DCGAN\n",
                "\n",
                "| Fitur | DCGAN | Conditional GAN (cGAN) |\n",
                "|---|---|---|\n",
                "| **Input Generator** | Noise acak ($z$). | Noise acak ($z$) + Label kelas ($y$). |\n",
                "| **Input Discriminator** | Gambar ($x$). | Gambar ($x$) + Label kelas ($y$). |\n",
                "| **Kontrol Output** | Acak. | Terkendali (berdasarkan label). |\n",
                "\n",
                "**cGAN** memungkinkan kita untuk memandu proses pembangkitan gambar dengan memberikan label sebagai input tambahan."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Persiapan Lingkungan, Import, dan GCS Auth\n",
                "\n",
                "Kita siapkan library dan autentikasi ke Google Cloud Storage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import matplotlib.pyplot as plt\n",
                "import sys, os\n",
                "import numpy as np\n",
                "import time as timer\n",
                "from tqdm import tqdm\n",
                "import grain.python as grain\n",
                "import optax\n",
                "import urllib.request\n",
                "import tarfile\n",
                "import pickle\n",
                "import safetensors\n",
                "from safetensors.flax import save_file\n",
                "\n",
                "from google.colab import auth\n",
                "from google.cloud import storage\n",
                "\n",
                "# Autentikasi GCS\n",
                "auth.authenticate_user()\n",
                "storage_client = storage.Client()\n",
                "BUCKET_NAME = 'dljax'\n",
                "bucket = storage_client.bucket(BUCKET_NAME)\n",
                "\n",
                "print(f\"JAX Device: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GCS Helpers\n",
                "def upload_to_gcs(local_path, gcs_path):\n",
                "    blob = bucket.blob(gcs_path)\n",
                "    blob.upload_from_filename(local_path)\n",
                "\n",
                "# Visualization helpers\n",
                "def set_grid(D, num_cells=1):\n",
                "    if len(D.shape) == 3:\n",
                "        n, h, w = D.shape\n",
                "        D = D[:, jnp.newaxis, :, :]\n",
                "    \n",
                "    if D.shape[1] in [1, 3] and D.shape[3] not in [1, 3]:\n",
                "        n, c, d1, d2 = D.shape\n",
                "    elif D.shape[3] in [1, 3]:\n",
                "        D = jnp.transpose(D, (0, 3, 1, 2))\n",
                "        n, c, d1, d2 = D.shape\n",
                "    else:\n",
                "        n, c, d1, d2 = D.shape\n",
                "    \n",
                "    grid_size = int(jnp.ceil(jnp.sqrt(num_cells)))\n",
                "    # Always use float32 for visualization grid\n",
                "    grid = jnp.zeros((c, grid_size * d1, grid_size * d2), dtype=jnp.float32)\n",
                "    \n",
                "    for i in range(num_cells):\n",
                "        if i >= n: break\n",
                "        r = i // grid_size\n",
                "        col = i % grid_size\n",
                "        grid = grid.at[:, r*d1:(r+1)*d1, col*d2:(col+1)*d2].set(D[i].astype(jnp.float32))\n",
                "        \n",
                "    return grid\n",
                "\n",
                "def normalize(x, new_min=0, new_max=255):\n",
                "    old_min = np.min(x)\n",
                "    old_max = np.max(x)\n",
                "    xn = (x - old_min) * ((new_max - new_min) / (old_max - old_min)) + new_min\n",
                "    return xn\n",
                "\n",
                "# Checkpoint helper with GCS Auto-Upload\n",
                "def save_checkpoint(model: nnx.Module, epoch: int, filedir: str = \"checkpoint\", gcs_prefix: str = \"models\"):\n",
                "    _, state = nnx.split(model)\n",
                "    flat_state = state.to_pure_dict()\n",
                "    \n",
                "    def flatten_dict(d, parent_key='', sep='.'):\n",
                "        items = []\n",
                "        for k, v in d.items():\n",
                "            if v is None: continue\n",
                "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
                "            if isinstance(v, dict):\n",
                "                items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
                "            else:\n",
                "                # Convert to float32 before saving to ensure compatibility\n",
                "                items.append((new_key, v.astype(jnp.float32) if hasattr(v, 'astype') else v))\n",
                "        return dict(items)\n",
                "    \n",
                "    flat_params = flatten_dict(flat_state)\n",
                "    \n",
                "    filename = f\"epoch_{epoch}.safetensors\"\n",
                "    local_dir = os.path.join(\"/content\", filedir)\n",
                "    os.makedirs(local_dir, exist_ok=True)\n",
                "    local_path = os.path.join(local_dir, filename)\n",
                "    save_file(flat_params, local_path)\n",
                "    \n",
                "    gcs_path = f\"{gcs_prefix}/{filedir}/{filename}\"\n",
                "    upload_to_gcs(local_path, gcs_path)\n",
                "    print(f\"Model saved locally to {local_path} and uploaded to gs://{BUCKET_NAME}/{gcs_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hyperparameters dan Direktori\n",
                "\n",
                "Kita definisikan konstanta yang akan digunakan selama eksperimen. Di sini kita menentukan `DTYPE` untuk mixed precision."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 64\n",
                "NUM_EPOCH = 100\n",
                "IMAGE_SIZE = 64\n",
                "NC = 3\n",
                "NZ = 100\n",
                "NGF = 64\n",
                "NDF = 64\n",
                "LR = 2e-4 \n",
                "BETA1 = 0.5 \n",
                "NVIZ = 64\n",
                "NUM_CLASSES = 10\n",
                "\n",
                "# Mixed Precision Setting\n",
                "DTYPE = jnp.bfloat16 # Gunakan bfloat16 untuk A100 (lebih stabil dari float16)\n",
                "\n",
                "DATA_DIR = \"/content/data\"\n",
                "SAMPLE_DIR = \"samples\"\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(SAMPLE_DIR, exist_ok=True)\n",
                "\n",
                "DATASET = 'cifar10'\n",
                "MODEL_DIR = \"models\"\n",
                "checkpoint_dir = os.path.join(MODEL_DIR, f\"cond-gan_{DATASET}_z{NZ}_bf16\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset: CIFAR-10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_and_extract_cifar10(dest_dir):\n",
                "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
                "    filename = os.path.join(dest_dir, \"cifar-10-python.tar.gz\")\n",
                "    extract_path = os.path.join(dest_dir, \"cifar-10-batches-py\")\n",
                "    if not os.path.exists(extract_path):\n",
                "        if not os.path.exists(filename):\n",
                "            print(f\"Downloading {url}...\")\n",
                "            urllib.request.urlretrieve(url, filename)\n",
                "        with tarfile.open(filename, \"r:gz\") as tar:\n",
                "            tar.extractall(path=dest_dir)\n",
                "    return extract_path\n",
                "\n",
                "def load_cifar10_local(data_dir):\n",
                "    def unpickle(file):\n",
                "        with open(file, 'rb') as fo:\n",
                "            d = pickle.load(fo, encoding='bytes')\n",
                "        return d\n",
                "    images, labels = [], []\n",
                "    for i in range(1, 6):\n",
                "        batch = unpickle(os.path.join(data_dir, f\"data_batch_{i}\"))\n",
                "        images.append(batch[b'data'])\n",
                "        labels.append(batch[b'labels'])\n",
                "    X_train = np.vstack(images)\n",
                "    y_train = np.hstack(labels).astype(np.int32)\n",
                "    return X_train, y_train\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "cifar_path = download_and_extract_cifar10(DATA_DIR)\n",
                "X_train_all, y_train_all = load_cifar10_local(cifar_path)\n",
                "print(f\"Data loaded: {X_train_all.shape} images.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CIFARSource(grain.RandomAccessDataSource):\n",
                "    def __init__(self, images, labels):\n",
                "        self._images, self._labels = images, labels\n",
                "    def __len__(self): return len(self._images)\n",
                "    def __getitem__(self, index):\n",
                "        from PIL import Image\n",
                "        img = self._images[index].reshape(3, 32, 32).transpose(1, 2, 0).astype(np.uint8)\n",
                "        img = Image.fromarray(img).resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR)\n",
                "        img = np.array(img).astype(np.float32)\n",
                "        image = (img / 255.0) * 2.0 - 1.0\n",
                "        return {'image': image, 'label': self._labels[index]}\n",
                "\n",
                "def create_loader(data_source, batch_size, shuffle=False, seed=0):\n",
                "    sampler = grain.IndexSampler(num_records=len(data_source), shard_options=grain.NoSharding(), shuffle=shuffle, num_epochs=1, seed=seed)\n",
                "    dataloader = grain.DataLoader(data_source=data_source, sampler=sampler, worker_count=0)\n",
                "    class BatchIterator:\n",
                "        def __init__(self, loader, batch_size, num_records): self.loader, self.batch_size, self.num_records = loader, batch_size, num_records\n",
                "        def __len__(self): return (self.num_records + self.batch_size - 1) // self.batch_size\n",
                "        def __iter__(self):\n",
                "            batch_images, batch_labels = [], []\n",
                "            for record in self.loader:\n",
                "                batch_images.append(record['image'])\n",
                "                batch_labels.append(record['label'])\n",
                "                if len(batch_images) == self.batch_size:\n",
                "                    yield np.stack(batch_images), np.array(batch_labels)\n",
                "                    batch_images, batch_labels = [], []\n",
                "            if batch_images: yield np.stack(batch_images), np.array(batch_labels)\n",
                "    return BatchIterator(dataloader, batch_size, len(data_source))\n",
                "\n",
                "train_loader = create_loader(CIFARSource(X_train_all, y_train_all), BATCH_SIZE, shuffle=True, seed=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Arsitektur Model (Mixed Precision Ready)\n",
                "\n",
                "Kita update model agar menggunakan `dtype` yang ditentukan (bf16) untuk komputasi, namun tetap menggunakan `float32` untuk parameter agar pelatihan stabil."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, num_classes, rngs: nnx.Rngs, dtype=jnp.float32):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        self.num_classes = num_classes\n",
                "        self.dtype = dtype\n",
                "        \n",
                "        # Komputasi dalam dtype (bf16/fp16), Parameter tetap fp32\n",
                "        c_args = {\"param_dtype\": jnp.float32, \"dtype\": dtype, \"kernel_init\": normal_init, \"use_bias\": False, \"rngs\": rngs}\n",
                "        \n",
                "        self.convt1 = nnx.ConvTranspose(nz + num_classes, ngf * 8, kernel_size=(4, 4), strides=(1, 1), padding='VALID', **c_args)\n",
                "        self.bn1 = nnx.BatchNorm(ngf * 8, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        \n",
                "        self.convt2 = nnx.ConvTranspose(ngf * 8, ngf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn2 = nnx.BatchNorm(ngf * 4, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        \n",
                "        self.convt3 = nnx.ConvTranspose(ngf * 4, ngf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn3 = nnx.BatchNorm(ngf * 2, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        \n",
                "        self.convt4 = nnx.ConvTranspose(ngf * 2, ngf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn4 = nnx.BatchNorm(ngf, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        \n",
                "        self.convt5 = nnx.ConvTranspose(ngf, nc, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "\n",
                "    def __call__(self, z, c, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None: use_running_average = not train\n",
                "        \n",
                "        # Cast inputs ke dtype\n",
                "        z = z.astype(self.dtype)\n",
                "        c = c.astype(self.dtype)\n",
                "        \n",
                "        h = jnp.concatenate([z, c], axis=1).reshape(-1, 1, 1, z.shape[1] + c.shape[1])\n",
                "        h = nnx.relu(self.bn1(self.convt1(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn2(self.convt2(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn3(self.convt3(h), use_running_average=use_running_average))\n",
                "        h = nnx.relu(self.bn4(self.convt4(h), use_running_average=use_running_average))\n",
                "        # Output tanh tetap float32 untuk stabilitas loss\n",
                "        return nnx.tanh(self.convt5(h)).astype(jnp.float32)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Discriminator(nnx.Module):\n",
                "    def __init__(self, nc, ndf, num_classes, rngs: nnx.Rngs, dtype=jnp.float32):\n",
                "        normal_init = nnx.initializers.normal(0.02)\n",
                "        self.dtype = dtype\n",
                "        c_args = {\"param_dtype\": jnp.float32, \"dtype\": dtype, \"kernel_init\": normal_init, \"use_bias\": False, \"rngs\": rngs}\n",
                "        \n",
                "        self.conv1 = nnx.Conv(nc + num_classes, ndf, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.conv2 = nnx.Conv(ndf, ndf * 2, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn2 = nnx.BatchNorm(ndf * 2, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        self.conv3 = nnx.Conv(ndf * 2, ndf * 4, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn3 = nnx.BatchNorm(ndf * 4, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        self.conv4 = nnx.Conv(ndf * 4, ndf * 8, kernel_size=(4, 4), strides=(2, 2), padding='SAME', **c_args)\n",
                "        self.bn4 = nnx.BatchNorm(ndf * 8, param_dtype=jnp.float32, dtype=dtype, rngs=rngs)\n",
                "        self.conv5 = nnx.Conv(ndf * 8, 1, kernel_size=(4, 4), strides=(1, 1), padding='VALID', **c_args)\n",
                "\n",
                "    def __call__(self, x, c, train: bool = True, use_running_average: bool = None):\n",
                "        if use_running_average is None: use_running_average = not train\n",
                "        \n",
                "        # Cast inputs ke dtype\n",
                "        x = x.astype(self.dtype)\n",
                "        c = c.astype(self.dtype)\n",
                "        \n",
                "        c_spatial = jnp.broadcast_to(c[:, None, None, :], (x.shape[0], x.shape[1], x.shape[2], c.shape[1]))\n",
                "        h = jnp.concatenate([x, c_spatial], axis=-1)\n",
                "        h = nnx.leaky_relu(self.conv1(h), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn2(self.conv2(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn3(self.conv3(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn4(self.conv4(h), use_running_average=use_running_average), negative_slope=0.2)\n",
                "        # Output float32\n",
                "        return self.conv5(h).flatten().astype(jnp.float32)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Logic Training (Mixed Precision)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CGAN(nnx.Module):\n",
                "    def __init__(self, nz, ngf, nc, ndf, num_classes, rngs: nnx.Rngs, dtype=jnp.float32):\n",
                "        self.netG = Generator(nz, ngf, nc, num_classes, rngs, dtype=dtype)\n",
                "        self.netD = Discriminator(nc, ndf, num_classes, rngs, dtype=dtype)\n",
                "\n",
                "rngs = nnx.Rngs(0)\n",
                "model = CGAN(NZ, NGF, NC, NDF, NUM_CLASSES, rngs=rngs, dtype=DTYPE)\n",
                "optimizerG = nnx.Optimizer(model.netG, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "optimizerD = nnx.Optimizer(model.netD, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "\n",
                "def loss_bce(logits, labels): return jnp.mean(optax.sigmoid_binary_cross_entropy(logits, labels))\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_D(model, optimizerD, real_x, c_vec, noise):\n",
                "    # G menghasilkan dalam bf16/fp16, tapi output dikembalikan ke fp32\n",
                "    fake_x = jax.lax.stop_gradient(model.netG(noise, c_vec, train=True))\n",
                "    def loss_fn(model):\n",
                "        real_logits = model.netD(real_x, c_vec, train=True)\n",
                "        fake_logits = model.netD(fake_x, c_vec, train=True)\n",
                "        loss = loss_bce(real_logits, jnp.ones_like(real_logits)) + loss_bce(fake_logits, jnp.zeros_like(fake_logits))\n",
                "        return loss, (nnx.sigmoid(real_logits), nnx.sigmoid(fake_logits))\n",
                "    (loss, (real_p, fake_p)), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    # Update tetap dalam fp32\n",
                "    optimizerD.update(model.netD, grads.netD)\n",
                "    return loss, jnp.mean(real_p), jnp.mean(fake_p)\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_G(model, optimizerG, c_vec, noise):\n",
                "    def loss_fn(model):\n",
                "        fake_x = model.netG(noise, c_vec, train=True)\n",
                "        fake_logits = model.netD(fake_x, c_vec, train=True)\n",
                "        loss = loss_bce(fake_logits, jnp.ones_like(fake_logits))\n",
                "        return loss, nnx.sigmoid(fake_logits)\n",
                "    (loss, outD), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
                "    optimizerG.update(model.netG, grads.netG)\n",
                "    return loss, jnp.mean(outD)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Loop Pelatihan (Main Loop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Starting Training Loop with {DTYPE} Mixed Precision...\")\n",
                "step_rng = jax.random.PRNGKey(0)\n",
                "fixed_latent = jax.random.normal(jax.random.PRNGKey(42), (NVIZ, NZ))\n",
                "fixed_y = jnp.array([i % NUM_CLASSES for i in range(NVIZ)])\n",
                "fixed_cvec = jax.nn.one_hot(fixed_y, NUM_CLASSES)\n",
                "\n",
                "for epoch in range(NUM_EPOCH):\n",
                "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}\") as tepoch:\n",
                "        for batch_idx, (real_x, y) in enumerate(tepoch):\n",
                "            c_vec = jax.nn.one_hot(y, NUM_CLASSES)\n",
                "            step_rng, rng_d, rng_g = jax.random.split(step_rng, 3)\n",
                "            \n",
                "            errD, D_x, D_G_z1 = train_step_D(\n",
                "                model, optimizerD, real_x, c_vec, \n",
                "                jax.random.normal(rng_d, (real_x.shape[0], NZ))\n",
                "            )\n",
                "            \n",
                "            errG, D_G_z2 = train_step_G(\n",
                "                model, optimizerG, c_vec, \n",
                "                jax.random.normal(rng_g, (real_x.shape[0], NZ))\n",
                "            )\n",
                "            \n",
                "            if batch_idx % 10 == 0:\n",
                "                tepoch.set_postfix(Loss_D=f\"{errD:.4f}\", Loss_G=f\"{errG:.4f}\", Dx=f\"{D_x:.4f}\", Dgz=f\"{D_G_z2:.4f}\")\n",
                "\n",
                "    # Visualisasi dan simpan ke GCS\n",
                "    fake_samples = model.netG(fixed_latent, fixed_cvec, train=False)\n",
                "    grid = set_grid(fake_samples, num_cells=NVIZ)\n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.imshow(np.transpose(np.array(normalize(grid, 0, 1)), (1, 2, 0)))\n",
                "    plt.axis('off')\n",
                "    \n",
                "    sample_name = f'samples_epoch_{epoch+1}.png'\n",
                "    sample_path = os.path.join(SAMPLE_DIR, sample_name)\n",
                "    plt.savefig(sample_path)\n",
                "    upload_to_gcs(sample_path, f\"{checkpoint_dir}/samples/{sample_name}\")\n",
                "    plt.show()\n",
                "\n",
                "    # Checkpointing ke GCS\n",
                "    save_checkpoint(model.netG, epoch + 1, filedir=\"generator\", gcs_prefix=checkpoint_dir)\n",
                "    save_checkpoint(model.netD, epoch + 1, filedir=\"discriminator\", gcs_prefix=checkpoint_dir)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}